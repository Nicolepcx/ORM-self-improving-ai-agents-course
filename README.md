⭐ If you find this repository helpful, please consider giving it a ⭐ here on GitHub (click the star button in the top right corner) 
It's a quick way to show support for this openly available code. ⭐

![OReilly_logo_rgb.png](resources%2FOReilly_logo_rgb.png)

# Develop Self-Improving AI Agents with Reinforcement Learning

This repository provides the hands-on excercises for the Live Event. It covers policy rollouts, reward modeling, trajectory generation, optimization methods, and tool-use training for agentic systems.

## Repository Structure

```
.
├── hands_on/  # Hands-on exercises     
└── README.md
```

---


---

## Contents and Exercises

### Section 1 • Foundations of RL for LLMs

Core concepts:

* Policies, rollouts, trajectories, rewards  
* Minimal reinforcement loop for LLMs  
* Relative feedback integration  

**Hands on notebook:**
[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Nicolepcx/ORM-self-improving-ai-agents-course/blob/main/hands_on/session_01_HANDS_ON_frozen_lake.ipynb)

---

### Section 2 • Reward Modeling and Relative Scoring

Core concepts:

* Reward shaping for LLM behavior  
* Relative ranking vs absolute scoring  
* RULER relative universal LLM elicited rewards  

**Hands on notebook:**
<a href="https://colab.research.google.com/github/Nicolepcx/ORM-self-improving-ai-agents-course/blob/main/hands_on/session_02_HANDS_ON_reward_function.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


---

### Section 3 • Agent Trajectories and Rollout Design

Core concepts:

* Agent Reinforcement Trainer rollout generation  
* Message sequences, actions, and feedback  
* Multi trajectory sampling for optimization  

**Hands on notebook:**
<a href="https://colab.research.google.com/github/Nicolepcx/ORM-self-improving-ai-agents-course/blob/main/hands_on/session_03_HANDS_ON_rock_paper_lizard_spock.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


---

### Section 4 • Optimization and Continuous Improvement

Core concepts:

* GRPO group relative policy optimization  
* GSPO group sequence policy optimization  
* Stabilizing RL for large models including MoE architectures  

**Hands on notebook:**
<a href="https://colab.research.google.com/github/Nicolepcx/ORM-self-improving-ai-agents-course/blob/main/hands_on/session_04_HANDS_ON_additional_histories.ipynb" target="_blank"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>


---

### Section 5 • Teaching Agents Tool Mastery (MCP Integration)

Core concepts:

* Automatic tool use training via structured task sets  
* Using RULER for tool use reward evaluation  
* MCP based agent to agent communication  

**Hands on notebook:**

<a target="_blank" href="https://colab.research.google.com/github/Nicolepcx/ORM-self-improving-ai-agents-course/blob/main/hands_on/session_05_HANDS_ON_MCP_server_Rube.ipynb">
  <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/>
</a>

---

## Running the Notebooks

All notebooks are designed to run in Google Colab without local setup.  
Click the corresponding Open in Colab button above to start an exercise.


